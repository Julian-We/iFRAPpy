{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bleach\n",
    "from importlib_metadata import version\n",
    "import numpy as np\n",
    "from numpy.core.fromnumeric import repeat\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from cycler import cycler\n",
    "import tifffile as tf\n",
    "import matplotlib.animation as animation\n",
    "from scipy.optimize import curve_fit, fsolve\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experiment():\n",
    "\n",
    "\n",
    "    def __init__(self, lsm_path, background_path, reference_path, activated_region_path, adjacent_region_path, export_path, batch_mode=True):\n",
    "        self.p_lsm_file = lsm_path\n",
    "        self.p_bckgrnd = background_path\n",
    "        self.p_ref = reference_path\n",
    "        self.p_act_granule = activated_region_path\n",
    "        self.p_other_gran = adjacent_region_path[0]\n",
    "        self.p_xprt = export_path\n",
    "        self.batch_mode = batch_mode\n",
    "        self.fps_export = 24\n",
    "        self.dict_json = {\n",
    "            'sample' : lsm_path,\n",
    "            'csv-paths' : {'lsm':lsm_path, 'activated-region':activated_region_path, 'reference':reference_path, 'background':background_path, 'adjacent': adjacent_region_path,'export': export_path},\n",
    "            'log' : []\n",
    "        }\n",
    "\n",
    "\n",
    "        try:\n",
    "            self.tfig_path = self.p_xprt +  os.path.sep + 'figs' + os.path.sep + 'transparant' + os.path.sep\n",
    "            os.makedirs(self.tfig_path) \n",
    "            self.fig_path = self.p_xprt +  os.path.sep + 'figs' + os.path.sep + 'wBackground' + os.path.sep\n",
    "            os.makedirs(self.fig_path)\n",
    "            self.excsv_path = self.p_xprt +  os.path.sep + 'csv' + os.path.sep\n",
    "            os.makedirs(self.excsv_path)  \n",
    "        except:\n",
    "            time.sleep(0.005)\n",
    "            self.tfig_path = self.p_xprt +  os.path.sep + 'figs' + os.path.sep + 'transparant' + os.path.sep\n",
    "            self.fig_path = self.p_xprt +  os.path.sep + 'figs' + os.path.sep + 'wBackground' + os.path.sep\n",
    "            self.excsv_path = self.p_xprt +  os.path.sep + 'csv' + os.path.sep\n",
    "        \n",
    "    def json_update(self, key, value):\n",
    "        if key.lower() == 'log':\n",
    "            self.dict_json['log'].append(f'[{datetime.now().strftime(\"%H:%M:%S\")}]: ' + value)\n",
    "        elif type(value).__module__ == np.__name__:\n",
    "            self.dict_json.update({key : value.tolist()})\n",
    "        elif type(value) in [np.dtype(np.int64).type, np.dtype(np.float64).type, type(pd.Int64Dtype())]:\n",
    "            self.dict_json.update({key : float(value)})\n",
    "        else:    \n",
    "            self.dict_json.update({key: value})\n",
    "        \n",
    "\n",
    "\n",
    "    def dataextractor(self, file_path):\n",
    "        # Opens the csv of the entered path and determins the needed columns \n",
    "        df = pd.read_csv(file_path)\n",
    "        df_columns = df.head()\n",
    "        try:\n",
    "\n",
    "            for abc in df_columns:\n",
    "                if 'IntDen' in abc and 'Raw' not in abc:\n",
    "                    intensity_column = abc\n",
    "                    self.json_update('Intensity column', intensity_column)\n",
    "                elif 'Ch' in abc:\n",
    "                    channel_column = abc\n",
    "                    #print('Channel Column: ' + channel_column)\n",
    "                elif 'ch' in abc:\n",
    "                    channel_column = abc\n",
    "                    #print('Channel Column: ' + channel_column)\n",
    "                elif 'Frame' in abc:\n",
    "                    frame_column = abc\n",
    "                    #print('Frame Column: ' + frame_column)\n",
    "                elif 'Area' in abc:\n",
    "                    area_column = abc\n",
    "                    #print('Area Column: ' + area_column)\n",
    "                elif 'area' in abc:\n",
    "                    area_column = abc\n",
    "        \n",
    "\n",
    "        except:\n",
    "            print('------------------------ Worng or missing Dimensions. Your measurements must contain: \"Ch\" and \"RawIntDen\" or \"IntDen\". Please check your measurement settings in Fiji/ImageJ! ------------------------')\n",
    "\n",
    "        df.rename(columns={intensity_column:'Int', channel_column:'Ch', frame_column:'Frame', area_column:'Area'}, inplace=True)\n",
    "        intensity_column = 'Int'\n",
    "        channel_column = 'Ch'\n",
    "        frame_column = 'Frame'\n",
    "        area_column = 'Area'\n",
    "\n",
    "        # Sorts the dataframe after channel and Frame in ascending order (theoretically unnessecary, but nice to have) and drops the old index\n",
    "        df_sorted = df.sort_values([channel_column, frame_column])\n",
    "        df_sorted.reset_index(drop=True, inplace=True)\n",
    "        # IMPORTANT: This is part of the normalization the fluorescent intensity units to the area of the ROI\n",
    "        df_sorted[intensity_column]= df_sorted[intensity_column]/df_sorted[area_column] \n",
    "        # Extracts the Intensity and Channels\n",
    "        df_sorted2 = df_sorted[[intensity_column, channel_column]]\n",
    "        # All datapoints for channel one and two sort\n",
    "        df_sorted2_pa = df_sorted2.loc[df_sorted2[channel_column] == 1]\n",
    "        out_pa = df_sorted2_pa.drop(columns=[channel_column])\n",
    "        df_sorted2_ch2 = df_sorted2.loc[df_sorted2[channel_column] == 2]\n",
    "        out_ch2 = df_sorted2_ch2.drop(columns=[channel_column])\n",
    "        # Extract area information\n",
    "        out_area = df_sorted[area_column].loc[df_sorted2[channel_column] == 1]\n",
    "        return out_ch2, out_pa, out_area\n",
    "\n",
    "    def dataextractor_frap(self, file_path):\n",
    "        # Opens the csv of the entered path and determins the needed columns \n",
    "        df = pd.read_csv(file_path)\n",
    "        df_columns = df.head()\n",
    "        try:\n",
    "            \n",
    "            for abc in df_columns:\n",
    "                if 'IntDen' in abc and 'Raw' not in abc:\n",
    "                    intensity_column = abc\n",
    "                    self.json_update('Intensity column', intensity_column)\n",
    "                elif 'Ch' in abc:\n",
    "                    channel_column = abc\n",
    "                    #print('Channel Column: ' + channel_column)\n",
    "                elif 'ch' in abc:\n",
    "                    channel_column = abc\n",
    "                    #print('Channel Column: ' + channel_column)\n",
    "                elif 'Area' in abc:\n",
    "                    area_column = abc\n",
    "                    #print('Area Column: ' + area_column)\n",
    "                elif 'area' in abc:\n",
    "                    area_column = abc\n",
    "                elif 'Frame' in abc or 'Slice' in abc or 'slice' in abc :\n",
    "                    frame_column = abc\n",
    "                    # print('Frame Column: ' + frame_column)\n",
    "\n",
    "        except:\n",
    "            print('------------------------ Worng or missing Dimensions. Your measurements must contain: \"Ch\" and \"RawIntDen\" or \"IntDen\". Please check your measurement settings in Fiji/ImageJ! ------------------------')\n",
    "\n",
    "        df.rename(columns={intensity_column:'Int', frame_column:'Frame', area_column:'Area'}, inplace=True)\n",
    "        intensity_column = 'Int'\n",
    "        frame_column = 'Frame'\n",
    "        area_column = 'Area'\n",
    "\n",
    "        # Sorts the dataframe after channel and Frame in ascending order (theoretically unnessecary, but nice to have) and drops the old index\n",
    "        df_sorted = df.sort_values(by=[frame_column])\n",
    "        df_sorted.reset_index(drop=True, inplace=True)\n",
    "        # IMPORTANT: This is part of the normalization the fluorescent intensity units to the area of the ROI\n",
    "        df_sorted[intensity_column]= df_sorted[intensity_column]/df_sorted[area_column] \n",
    "        # Extracts the Intensity and Channels\n",
    "        df_sorted2 = df_sorted[[intensity_column]]\n",
    "        # All datapoints for channel one and two sort\n",
    "        out_pa = df_sorted2[[intensity_column]]\n",
    "        out_ch2 = out_pa\n",
    "        # Extract area information\n",
    "        out_area = df_sorted[area_column]\n",
    "        return out_ch2, out_pa, out_area\n",
    "\n",
    "    def extract_and_normalize(self):\n",
    "        try:\n",
    "            print(self.p_act_granule, '\\n',self.p_bckgrnd, '\\n',self.p_ref, '\\n',self.p_other_gran)\n",
    "            _, analyze_out, analyze_area = self.dataextractor(self.p_act_granule)\n",
    "            print('active')\n",
    "            _, background_out, background_area = self.dataextractor(self.p_bckgrnd)\n",
    "            print('background')\n",
    "            _, reference_out, reference_area = self.dataextractor(self.p_ref)\n",
    "            print('ref')\n",
    "            _, adjacent_out, adjacent_area = self.dataextractor(self.p_other_gran)\n",
    "            print('othergran')\n",
    "        except:\n",
    "            _, analyze_out, analyze_area = self.dataextractor_frap(self.p_act_granule)\n",
    "            _, background_out, background_area = self.dataextractor_frap(self.p_bckgrnd)\n",
    "            _, reference_out, reference_area = self.dataextractor_frap(self.p_ref)\n",
    "            _, adjacent_out, adjacent_area = self.dataextractor_frap(self.p_other_gran)\n",
    "\n",
    "        ## Normalization procedures\n",
    "        # Subtract background from the images\n",
    "        analyzed = analyze_out - background_out\n",
    "        reference = reference_out - background_out\n",
    "        adjacent = adjacent_out - background_out\n",
    "        # Main normalization\n",
    "        norm = (analyzed/reference)*(np.average(analyzed[:self.bleach_start_frame])/np.average(reference[:self.bleach_start_frame]))\n",
    "        # print('Background multiplicator',np.average(analyzed[:4])/np.average(reference[:4]))\n",
    "        # Setting absolute values between 0 and 1\n",
    "        norm_abs = norm['Int'] - norm['Int'][self.bleach_end_frame]\n",
    "        norm_abs = norm_abs/np.average(norm_abs[:self.bleach_start_frame])\n",
    "        self.active_norm = norm_abs\n",
    "        self.json_update('double normalized relative data', norm_abs.to_dict())\n",
    "        self.json_update('log', 'Data was extracted from csv files and Phair double normalization was performed')\n",
    "\n",
    "        norm_adjacent = (adjacent/reference)*(np.average(adjacent[:self.bleach_start_frame])/np.average(reference[:self.bleach_start_frame]))\n",
    "        # norm_abs_adjacent = norm_adjacent['Int'] - norm_adjacent.loc[self.bleach_end_frame]\n",
    "        # norm_abs_adjacent = norm_abs_adjacent/np.average(norm_abs_adjacent[:self.bleach_start_frame])\n",
    "        \n",
    "        # temporary_fix\n",
    "        norm_abs_adjacent = norm_adjacent\n",
    "        self.adjacent_norm = norm_abs_adjacent\n",
    "\n",
    "        self.json_update('log', 'Normalization for adjacent granules was performed')\n",
    "\n",
    "        self.xprt_csv = pd.DataFrame(data=self.timesteps, columns=['time'])\n",
    "        \n",
    "        self.xprt_csv['active raw'] = analyze_out\n",
    "        self.xprt_csv['active norm'] = norm_abs\n",
    "        self.xprt_csv['background'] = background_out\n",
    "        self.xprt_csv['reference'] = reference\n",
    "        self.xprt_csv['area active'] = analyze_area\n",
    "        self.xprt_csv['adjacent'] = norm_abs_adjacent\n",
    "        self.xprt_csv['area adjacent'] = adjacent_area\n",
    "        \n",
    "    def metadata(self):\n",
    "        with tf.TiffFile(self.p_lsm_file[0]) as og_image:\n",
    "            volume = og_image.asarray()\n",
    "            axes = og_image.series[0].axes\n",
    "            metadata = og_image.lsm_metadata\n",
    "        \n",
    "        for (stamp, onoff, __) in metadata['EventList']:\n",
    "            if onoff==2:\n",
    "                bleach_start_time = stamp\n",
    "            elif onoff == 3:\n",
    "                bleach_end_time = stamp\n",
    "\n",
    "        # Create a time axis\n",
    "        self.timesteps = np.array(metadata['TimeStamps']) - bleach_end_time # Converts the timesteps in to a numpy array\n",
    "        self.bleach_start_time = bleach_start_time - bleach_end_time\n",
    "        self.bleach_end_time = bleach_end_time - bleach_end_time\n",
    "        self.bleach_end_frame = (np.abs(self.timesteps - self.bleach_end_time)).argmin()\n",
    "        self.bleach_start_frame = self.bleach_end_frame - 1\n",
    "\n",
    "        dict_pa_metadata = {\n",
    "            'time start' : self.bleach_start_time,\n",
    "            'time end' : self.bleach_end_time,\n",
    "            'index start' : float(self.bleach_start_frame),\n",
    "            'index end' : float(self.bleach_end_frame)\n",
    "        }\n",
    "\n",
    "        self.json_update('photoactivation metadata', dict_pa_metadata)\n",
    "        self.json_update('time steps', self.timesteps)\n",
    "        self.json_update('log', 'Image metadata was extracted')\n",
    "\n",
    "    \n",
    "    def PlotAndSave(self):\n",
    "        # Creates a figure with 2 subfigures, featuring the activated ROI in one and the other granules in the other\n",
    "        fig, ax = plt.subplots(2,1, figsize=(15,20))\n",
    "        ax[0].plot(self.timesteps, self.active_norm, color='#ed406c', label='Activated region') #, color='#292d34'\n",
    "        ax[0].axvspan(self.bleach_start_time, self.bleach_end_time, alpha=0.4, color='#3498db')\n",
    "        ax[0].set_xlabel('Time post photoactivation [s]')\n",
    "        ax[0].set_ylabel(r'Relative fluorescent intensity $F_{norm}/F_{max}$')\n",
    "        ax[0].set_title('Fluorescent signal in photoactivated region')\n",
    "        # Next subplots\n",
    "        ax[1].plot(self.timesteps, self.adjacent_norm, label='adjecent granule') #, color='#292d34'\n",
    "        ax[1].axvspan(self.bleach_start_time, self.bleach_end_time, alpha=0.4, color='#3498db')\n",
    "        ax[1].set_xlabel('Time post photoactivation [s]')\n",
    "        ax[1].set_ylabel(r'Relative fluorescent intensity $F_{norm}/F_{max}$')\n",
    "        ax[1].set_title('Fluorescent signal in secondary granules')\n",
    "        timestep_len = len(self.timesteps)\n",
    "        last_timestep = self.timesteps[timestep_len - 1]\n",
    "        ax[1].legend()\n",
    "        fig1_name = 'intenstity_curves'\n",
    "        plt.savefig(self.tfig_path + fig1_name + '_transparent' + '.pdf', transparent=True)\n",
    "        plt.savefig(self.fig_path + fig1_name + '.pdf')\n",
    "        if self.batch_mode == False:\n",
    "            plt.show()\n",
    "        elif self.batch_mode == True:\n",
    "            plt.ioff()\n",
    "            plt.close()\n",
    "        # Create the control panel - plots the timelapses from the control ROIs\n",
    "        fig3, ax3 = plt.subplots(2,2, figsize=(15,11))\n",
    "        \n",
    "        fig3.suptitle('Area normalized fluorescent intensities - controls', fontsize=42)\n",
    "\n",
    "        ax3[0,0].plot(self.timesteps, self.xprt_csv['active raw'], label='Photoactivated ROI')\n",
    "        ax3[0,0].set_ylabel(r'Fluorescent intensitie per area $F/A_{ROI}$') \n",
    "        ax3[0,0].set_xlabel(r'Time post photoactivation [sec]')\n",
    "        ax3[0,0].set_title('RAW fluorescent units per area of activated region')\n",
    "\n",
    "        ax3[1,0].plot(self.timesteps, self.xprt_csv['background'], label='Background')\n",
    "        ax3[1,0].set_ylabel(r'Fluorescent intensitie per area $F/A_{ROI}$') \n",
    "        ax3[1,0].set_xlabel(r'Time post photoactivation [sec]')\n",
    "        ax3[1,0].set_title('RAW fluorescent units per area of background activity')\n",
    "        # ax3[1,0].set_ylim(np.mean(self.xprt_csv['background'][0] - 6*np.std(self.xprt_csv['background'])[0],np.mean(self.xprt_csv['background'])[0] + 6*np.std(self.xprt_csv['background'])[0]))\n",
    "\n",
    "        ax3[0,1].plot(self.timesteps, self.xprt_csv['reference'], label='Reference')\n",
    "        ax3[0,1].set_ylabel(r'Fluorescent intensitie per area $F/A_{ROI}$') \n",
    "        ax3[0,1].set_xlabel(r'Time post photoactivation [sec]')\n",
    "        ax3[0,1].set_title('RAW fluorescent units per area of the whole cell (Reference)')\n",
    "\n",
    "        ax3[1,1].plot(self.timesteps, self.active_norm, label='Normalized and relative to first frame post activation')\n",
    "        ax3[1,1].set_ylabel(r'Fluorescent intensitie per area $F/A_{ROI}$') \n",
    "        ax3[1,1].set_xlabel(r'Time post photoactivation [sec]')\n",
    "        ax3[1,1].set_title('Normalized but not relative')\n",
    "        fig2_name = 'plots_controls'\n",
    "        plt.savefig(self.tfig_path + fig2_name + '_transparent' + '.pdf', transparent=True)\n",
    "        plt.savefig(self.fig_path + fig2_name + '.pdf')\n",
    "        if self.batch_mode == False:\n",
    "            plt.show()\n",
    "        elif self.batch_mode == True:\n",
    "            plt.ioff()\n",
    "            plt.close()\n",
    "\n",
    "        self.json_update('log', 'Plots created and saved')\n",
    "    \n",
    "    \n",
    "    def animationplot(self):\n",
    "        # Animated Figure of the photoactivated region\n",
    "        plt.style.use('ggplot')\n",
    "        fig2, ax2 = plt.subplots(figsize=(12,7))\n",
    "        def animate(i):\n",
    "            xs = self.xprt_csv['time']\n",
    "            ys = self.xprt_csv['active norm']\n",
    "            ax2.cla()\n",
    "            ax2.plot(xs[:i],ys[:i], label='Activated granule', color='#ed406c')\n",
    "            ax2.set_xlim([xs.min()+xs[0],xs.max()+0.025*xs.max()])\n",
    "            ax2.set_ylim([ys.min()-0.025*ys.max(),ys.max()+0.025*ys.max()])\n",
    "            ax2.set_title('Fluorescent decay by diffusion')\n",
    "            ax2.set_xlabel('Time post photoactivation [s]')\n",
    "            ax2.set_ylabel(r'Relative fluorescent intensity $F/F_{max}$')\n",
    "            ax2.axvspan(self.bleach_start_time, self.bleach_end_time, alpha=0.5, color='#3498db')\n",
    "        #Animation command and plot of figure\n",
    "        ani = animation.FuncAnimation(fig2, animate, interval=1000/self.fps_export, save_count=len(self.xprt_csv['time']))\n",
    "        giff = animation.PillowWriter(fps=self.fps_export)\n",
    "        ani.save(self.fig_path + '/animated_graph_active.gif', writer=giff)\n",
    "        if self.batch_mode == False:\n",
    "            plt.show(fig2)\n",
    "        elif self.batch_mode ==True:\n",
    "            plt.ioff()\n",
    "            plt.close()\n",
    "        self.json_update('log', 'Animation plot created and saved')\n",
    "\n",
    "\n",
    "    def create_fit_data(self):\n",
    "        \n",
    "        self.bleach_end_frame = abs(self.timesteps - self.bleach_end_time).argmin()\n",
    "        # Uses the export .csv-file as a source for the data and creates new variables one could use for\n",
    "        xdata = self.timesteps\n",
    "        xdata = xdata[self.bleach_end_frame:]\n",
    "        ydata = self.active_norm.values\n",
    "        ydata = ydata[self.bleach_end_frame:]\n",
    "        \n",
    "        try:\n",
    "            # Function of a two phased exponential decay, which is used for fitting\n",
    "            def func(x, a, b, c, d, e):\n",
    "                return a * np.exp(-b * x) + c * np.exp(-d * x) + e\n",
    "\n",
    "            # Function that uses the parametes from fitting, an x value and  according equation to calculate y\n",
    "            def biphase_decay_func(c, x):\n",
    "                fit = c[0]*np.exp(-c[1]*x) + c[2]*np.exp(-c[3]*x) + c[4]    # Function of two phased decay\n",
    "                return fit\n",
    "\n",
    "            # Fitting algorithm that fits the function above to the data\n",
    "            c, ccov = curve_fit(func, xdata, ydata)\n",
    "            \n",
    "            self.json_update('log', 'Biphasic exponential fit applied')\n",
    "\n",
    "            fit_parameters = dict({'a1': c[0],'k1': c[1],'a2': c[2],'k2':c[3],'b':c[4]})    # Send the parameters to a dictionary (reduces clutter)\n",
    "\n",
    "            # Calculates x and y values for the fitted curve\n",
    "            fit_x = np.linspace(xdata.min(), xdata.max(), len(xdata))    # Creates numpy aray with limits of the dataset for calculating the fitted curve\n",
    "            fit_y = biphase_decay_func(c, fit_x)                         # Creates y-values for the fitted curve using the \n",
    "\n",
    "            # Add r square of the data and fitted curve to the parameter dict\n",
    "            fit_parameters.update({'rsq': r2_score(ydata, fit_y)})\n",
    "\n",
    "            # Calculate the y intersection and the y-value of the halftime\n",
    "            fit_parameters.update({'y0' : biphase_decay_func(c, 0)})           # Calculates Y value of fit at x=0 and adds it to the parameters dict\n",
    "            y_half = (fit_parameters['y0']-fit_parameters['b'])/2              # Calculated the y-value for t(half)\n",
    "\n",
    "            fit_parameters.update({'percentFast' : (100*c[0])/(fit_parameters['y0']-fit_parameters['b'])})  # Adds the 'percentFast'-value from Prism to the dict\n",
    "\n",
    "            # Function supplies the equations to calculate the intercept of y(halftime) and the fitted curve\n",
    "            def f(z):\n",
    "                \n",
    "                x = z[0]\n",
    "                y = z[1]\n",
    "                ff = np.zeros(2)\n",
    "                # x = (np.log(y)-np.log(c[4]))/((-c[1]*np.log(c[0]))+(-c[3]*np.log(c[2])))\n",
    "                ff[0] = c[0]*np.exp(-c[1]*x) + c[2]*np.exp(-c[3]*x) + c[4] - y\n",
    "                ff[1] = y_half - y\n",
    "                # x = (np.log(y/(c[4]*c[0]*c[2])))/(-c[1]-c[3])\n",
    "\n",
    "                return ff\n",
    "        \n",
    "            # Solves for the intersection of y(halftime) and the fitted curve function\n",
    "            fit_parameters['t(1/2) mixed'], trash = fsolve(f, [2 ,y_half])\n",
    "            if fit_parameters.get('k1')>fit_parameters.get('k2'):\n",
    "                fit_parameters['t(1/2) fast'] = np.log(2)/fit_parameters.get('k1')\n",
    "                fit_parameters['t(1/2) slow'] = np.log(2)/fit_parameters.get('k2')\n",
    "                \n",
    "                fit_parameters['tau fast'] = 1/fit_parameters.get('k1')\n",
    "                fit_parameters['tau slow'] = 1/fit_parameters.get('k2')\n",
    "\n",
    "            else:\n",
    "                fit_parameters['t(1/2) fast'] = np.log(2)/fit_parameters.get('k2')\n",
    "                fit_parameters['t(1/2) slow'] = np.log(2)/fit_parameters.get('k1')\n",
    "\n",
    "                fit_parameters['tau fast'] = 1/fit_parameters.get('k2')\n",
    "                fit_parameters['tau slow'] = 1/fit_parameters.get('k1')\n",
    "\n",
    "            self.json_update('log', 'parameters calculated from fit')\n",
    "\n",
    "            # Plotting the intestity data as scatter and the fitting as a line plot\n",
    "            plt.style.use('ggplot')\n",
    "            fig_fit, ax_fit = plt.subplots(figsize=(12,7))\n",
    "            ax_fit.scatter(xdata, ydata, color=['#292d34'], label='Original data',alpha=0.4, s=10)\n",
    "            ax_fit.plot(fit_x,fit_y,'r-', label=f'Two phase exponental decay fit \\n $R^2 = $ {round(fit_parameters[\"rsq\"], 4)}')\n",
    "            ax_fit.set_title('Fluorescent decay by diffusion')\n",
    "            ax_fit.set_xlabel('Time post photoactivation [s]')\n",
    "            ax_fit.set_ylabel(r'Relative fluorescent intensity $F/F_{max}$')\n",
    "            fit_legend = ax_fit.legend()\n",
    "            # ax_fit.axhline()\n",
    "            #ax_fit.axvline(fit_parameters['t(1/2) mixed'],color='black', linestyle=':', label='$t_{1/2}$')\n",
    "            ax_fit.axvline(fit_parameters['t(1/2) slow'],color='black', linestyle=':', label='$t_{1/2}$')\n",
    "            # ax_fit.table(cellText=np.array(fit_parameters.values()), rowLabels=np.array(fit_parameters.keys()))\n",
    "            if self.batch_mode == False:\n",
    "                plt.show(fig_fit)\n",
    "            elif self.batch_mode ==True:\n",
    "                plt.ioff()\n",
    "                plt.close()\n",
    "            # Saving procedure\n",
    "            fig3_name = 'fitting'\n",
    "            plt.savefig(self.tfig_path + fig3_name + '_transparent' + '.pdf', transparent=True)\n",
    "            plt.savefig(self.fig_path + fig3_name + '.pdf')\n",
    "            \n",
    "            self.json_update('log', 'Fit plot created')\n",
    "            # There must be a more elegant way, but.....This loads the activated csv and extracts the area inforation out of it\n",
    "            active_area = pd.read_csv(self.p_act_granule)\n",
    "            active_area_columns = active_area.head()\n",
    "            for abc in active_area_columns:\n",
    "                if 'Area' in abc:\n",
    "                    area_column = abc\n",
    "                elif 'area' in abc:\n",
    "                    area_column = abc\n",
    "\n",
    "            #Checks if always the same area was used and if that was the case\n",
    "            def is_unique(s):\n",
    "                a = s.values\n",
    "                return (a[0] == a).all()\n",
    "            #print(is_unique(active_area[area_column]))\n",
    "            if is_unique(active_area[area_column]) == True:\n",
    "                area = active_area[area_column].iloc[0]\n",
    "            else:\n",
    "                area = 1\n",
    "                print('No Area found')\n",
    "            #print('Area is: ' + str(area))\n",
    "\n",
    "            # Calculates the diffioncoefficient using the halftime\n",
    "            def calculate_dcoef(t_half, area):\n",
    "                dcoef = 0.25*(area/np.pi)*t_half\n",
    "                return dcoef\n",
    "            fit_parameters.update({'D mixed' : calculate_dcoef(fit_parameters['t(1/2) mixed'], area)})\n",
    "            fit_parameters.update({'D slow' : calculate_dcoef(fit_parameters['t(1/2) slow'], area)})\n",
    "            \n",
    "            # Export the parameters form the curve fitting and following calculations and save them as .csv\n",
    "            fit_parameters_csv = pd.DataFrame.from_dict(fit_parameters, orient='index', columns=['values'])     # Create pandas dataframe from the parameter dictonary\n",
    "            fit_parameters_csv.to_csv(self.excsv_path + 'fit_parameters.csv')                                # Saves the dataframe as csv under the export path\n",
    "\n",
    "        except Exception as inst:\n",
    "            print('An error accured!')\n",
    "            print(inst)\n",
    "            fit_parameters = dict({'a1': 'NaN','k1': 'NaN','a2': 'NaN','k2':'NaN','b':'NaN', 'rsq': 'NaN', 'percentFast' : 'NaN'}) # Sets all as NaN\n",
    "        self.fitparameters = fit_parameters\n",
    "        self.json_update('fit parameters', fit_parameters)\n",
    "        self.json_update('log', 'fit parameters saved')\n",
    "        \n",
    "\n",
    "    def main(self, animated_graphs=False):\n",
    "        # Extracts metadata from lsm file\n",
    "        self.metadata()\n",
    "        \n",
    "        # Extracts fluorescent intensity data and normalizes it\n",
    "        self.extract_and_normalize()\n",
    "        \n",
    "        # Applies a two phase exponential fit to the data       \n",
    "        self.create_fit_data()\n",
    "        \n",
    "        # Creats plots for quality control\n",
    "        self.PlotAndSave()\n",
    "\n",
    "        #Checks if animated graphs are needed and if so, creates it\n",
    "        if animated_graphs == True:\n",
    "            self.animationplot()\n",
    "\n",
    "        with open(self.p_lsm_file[0].replace('.lsm', '.json'), 'w') as json_file:\n",
    "            json.dump(self.dict_json, json_file, indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error classes\n",
    "class ExperimentExistError(Exception):\n",
    "    \"\"\"This error is raised when a experiment is referenced that doesn't exist\"\"\"\n",
    "    def __init__(self, experiment, thelist):\n",
    "        self.message = f'The chosen experiment \"{experiment}\" was not defined before. Only experiments from the following list can be entered'\n",
    "        for experiment in thelist:\n",
    "            self.message += f'\\n {str(experiment)}'\n",
    "        super().__init__(self.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentGroup():\n",
    "    def __init__(self, csv, ctrl_csv=None, ctrl_name='Control', poi_name='Protein of interest', batch_file_mode=False):\n",
    "        self.batch_csv = csv\n",
    "        self.mw = 0\n",
    "        self.disorder_percent = 0\n",
    "        self.export_path = os.path.join(csv[:csv.rfind(os.path.sep)])\n",
    "        self.ctrl_csv = ctrl_csv\n",
    "        self.ctrl_name = ctrl_name\n",
    "        self.poi_name = poi_name\n",
    "        self.json_masterfile = {\n",
    "            'script-version' : '3.2.1',\n",
    "            'batchfile' : csv,\n",
    "            'log' : [],\n",
    "            'experiments' : [] # This should be the last line\n",
    "        }\n",
    "        self.dict_experiments = {}\n",
    "    def to_json(self):\n",
    "        try:\n",
    "            # This fuction is supposed to be passed at the end of ever other function to be certain ensure proper documentation\n",
    "            with open(self.batch_csv.replace('.csv', '.json'), 'w') as json_file:\n",
    "                json.dump(self.json_masterfile, json_file, indent=3)\n",
    "        except IsADirectoryError:\n",
    "            name = self.json_masterfile['Name'].replace(' ', '_')\n",
    "            with open(os.path.join(self.batch_csv[:self.batch_csv.rfind(os.path.sep)], f'json-file_{name}.json'), 'w') as json_file:\n",
    "                json.dump(self.json_masterfile, json_file, indent=3)\n",
    "            \n",
    "    def iterator(self, root_path):\n",
    "        for rootdir, folderdir, filedir in os.walk(root_path):\n",
    "            if rootdir == root_path:\n",
    "                samples = folderdir\n",
    "            if os.path.basename(rootdir) in samples:\n",
    "                sample_paths = {\n",
    "                    'adjacent-path' : None\n",
    "                }\n",
    "                for files in filedir:\n",
    "                    if files.endswith('.lsm') and files.startswith('.') == False:\n",
    "                        sample_paths.update({'lsm-path': os.path.join(rootdir, files)})\n",
    "                for csvfiles in os.listdir(os.path.join(rootdir,'csv')):\n",
    "                    if csvfiles.endswith('.csv') == True:\n",
    "                        if 'activ' in csvfiles and csvfiles.startswith('.') == False:\n",
    "                            sample_paths.update({'activated-path': os.path.join(rootdir,'csv', csvfiles)})\n",
    "                        elif 'ref' in csvfiles and csvfiles.startswith('.') == False:\n",
    "                            sample_paths.update({'reference-path': os.path.join(rootdir,'csv', csvfiles)})\n",
    "                        elif 'back' in csvfiles and csvfiles.startswith('.') == False:\n",
    "                            sample_paths.update({'background-path': os.path.join(rootdir,'csv', csvfiles)})\n",
    "                        elif 'adj' or 'sec' in csvfiles and csvfiles.startswith('.') == False:\n",
    "                            sample_paths.update({'adjacent-path': os.path.join(rootdir,'csv', csvfiles)})\n",
    "                if sample_paths['adjacent-path'] == None:\n",
    "                    sample_paths.update({'adjacent-path': sample_paths['activated-path']})\n",
    "                sample_paths.update({'export-path': os.path.join(rootdir, 'xprt')})\n",
    "                try:\n",
    "                    os.makedirs(sample_paths['export-path'])\n",
    "                except FileExistsError:\n",
    "                    pass\n",
    "                # print(sample_paths)\n",
    "                exp = experiment([sample_paths['lsm-path']], sample_paths['background-path'], sample_paths['reference-path'], sample_paths['activated-path'], [sample_paths['adjacent-path']], sample_paths['export-path'])\n",
    "                exp.main()\n",
    "                self.json_update('log', f'Sample {os.path.basename(sample_paths[\"lsm-path\"])} was processed')\n",
    "                # print(f'Finished processing file {index+1} of {len(list(batch_csv_file))}')\n",
    "                self.json_masterfile['experiments'].append(exp.dict_json)\n",
    "                clear_output()\n",
    "                self.to_json()\n",
    "        self.to_json()\n",
    "                \n",
    "                \n",
    "    \n",
    "    def iterate(self, csv_file=None):\n",
    "        csv_file = self.batch_csv if csv_file == None else self.batch_csv\n",
    "        with open(csv_file, newline='\\n') as bc:\n",
    "            batch_csv_file = csv.reader(bc, delimiter=',')\n",
    "            header = next(batch_csv_file)\n",
    "            for i, s in enumerate(header):\n",
    "                if 'lsm' in s:\n",
    "                    lsm_column = i\n",
    "                elif 'back' in s:\n",
    "                    bckgrnd_column = i \n",
    "                elif 'other' in s:\n",
    "                    other_column = i\n",
    "                elif 'activ' in s:\n",
    "                    activ_column = i\n",
    "                elif 'reference' in s:\n",
    "                    ref_column = i\n",
    "                elif 'export' in s or 'xprt' in s:\n",
    "                    xprt_column = i\n",
    "\n",
    "\n",
    "            index = 0\n",
    "            for csrow in batch_csv_file:\n",
    "                print('Processing: ', csrow[lsm_column])\n",
    "                for entry in csrow:\n",
    "                    if entry == '':\n",
    "                        for root_d, dirs, files in os.walk(csrow[lsm_column][:csrow[lsm_column].rfind(os.path.sep)]):\n",
    "                            for name in files:\n",
    "                                if 'activ' in name and name.endswith('.csv'):\n",
    "                                    csrow[activ_column] = os.path.join(root_d,name)\n",
    "                                if 'ref' in name and name.endswith('.csv'):\n",
    "                                    csrow[ref_column] = os.path.join(root_d,name)\n",
    "                                if 'back' in name and name.endswith('.csv'):\n",
    "                                    csrow[bckgrnd_column] = os.path.join(root_d,name)\n",
    "                            if 'xprt' in root_d[root_d.rfind(os.path.sep):]:\n",
    "                                csrow[xprt_column] = root_d\n",
    "                                self.json_update('log', f'root and therefore export path is {root_d}')\n",
    "\n",
    "                exp = experiment([csrow[lsm_column]], csrow[bckgrnd_column], csrow[ref_column], csrow[activ_column], [csrow[other_column]], csrow[xprt_column])\n",
    "                exp.main()\n",
    "                self.json_update('log', f'Sample {csrow[lsm_column][:csrow[lsm_column].rfind(os.path.sep)]} was processed')\n",
    "                # print(f'Finished processing file {index+1} of {len(list(batch_csv_file))}')\n",
    "                index += 1\n",
    "                self.json_masterfile['experiments'].append(exp.dict_json)\n",
    "                clear_output()\n",
    "                self.to_json()\n",
    "            \n",
    "            \n",
    "        # Document in a json-file\n",
    "        self.to_json()\n",
    "        print('done iterating')\n",
    "\n",
    "\n",
    "    def  calculate_diffusivity(self):\n",
    "        i=0\n",
    "        # Align all dataframes to line 10 for time and data\n",
    "        for experiment in self.json_masterfile['experiments']:\n",
    "            file_path = experiment['sample'][0]\n",
    "            file_name = file_path[file_path.rfind(os.path.sep)+1:file_path.rfind('.')]\n",
    "            shift = 9-int(experiment['photoactivation metadata']['index start'])\n",
    "            df_experiment = pd.DataFrame({'time steps':experiment['time steps'], file_name : experiment['double normalized relative data'].values()})\n",
    "            self.bad_boy_list = []\n",
    "            if df_experiment[file_name][10:].max() < 1.75 and df_experiment[file_name][10:].min() > 0:\n",
    "                if i==0:\n",
    "                    df_sum_data = pd.DataFrame(df_experiment[file_name].shift(shift))#.shift(max_id)\n",
    "                else:\n",
    "                    df_sum_data = pd.concat([df_sum_data, df_experiment[file_name].shift(shift)], axis=1)#.shift(max_id)\n",
    "\n",
    "                if i==0:\n",
    "                    df_sum_time = pd.DataFrame(df_experiment['time steps'].shift(shift))#.shift(max_id)\n",
    "                else:\n",
    "                    df_sum_time = pd.concat([df_sum_time, df_experiment['time steps'].shift(shift)], axis=1)#.shift(max_id)\n",
    "                i+=1\n",
    "            else:\n",
    "                self.bad_boy_list.append(df_experiment)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate the median per time point\n",
    "        time_steps = df_sum_time.T.describe().T['50%'].values\n",
    "        sample_count = df_sum_data.T.describe()[10]['count']\n",
    "\n",
    "        df_sum_data_desc = df_sum_data.T.describe().T\n",
    "        for index, row in df_sum_data_desc.iterrows():\n",
    "            if index > 10 and row['count'] < df_sum_data_desc['count'][10]:\n",
    "                self.row_break_index = index\n",
    "                break\n",
    "            else:\n",
    "                self.row_break_index = None\n",
    "        df_sum_data_desc['sem']=df_sum_data_desc['std']/np.sqrt(df_sum_data_desc['count'])\n",
    "        df_sum_data_desc['conf low'] = df_sum_data_desc['mean'] - df_sum_data_desc['sem']#*1.96\n",
    "        df_sum_data_desc['conf high'] = df_sum_data_desc['mean'] + df_sum_data_desc['sem']#*1.96\n",
    "\n",
    "\n",
    "        # Data for scatter plot and fitting\n",
    "        xdata_fit = time_steps[10:self.row_break_index]\n",
    "        ydata_fit = df_sum_data_desc['mean'][10:self.row_break_index].values\n",
    "        # plt.plot(xdata_fit, ydata_fit)\n",
    "        # plt.show()\n",
    "        self.json_update('log', 'Data of all experiment has been collected')\n",
    "        # Function of a two phased exponential decay, which is used for fitting\n",
    "        def func(x, a, b, c, d):\n",
    "            return -a * np.exp(-b * x) + -c * np.exp(-d * x) + (a+c)\n",
    "\n",
    "        # Function that uses the parametes from fitting, an x value and  according equation to calculate y\n",
    "        def biphase_decay_func(c, x):\n",
    "            fit = -c[0]*np.exp(-c[1]*x) + -c[2]*np.exp(-c[3]*x) + (c[0] + c[2])    # Function of two phased decay\n",
    "            return fit\n",
    "\n",
    "        # Fitting algorithm that fits the function above to the data\n",
    "        # try:\n",
    "        c, ccov = curve_fit(func, xdata_fit, ydata_fit, p0=[0.5, 0.1, 0.5, 0.1], bounds=[0, 3])\n",
    "        # except Exception as e:# RuntimeWarning or RuntimeError:\n",
    "        # print(f'of {e}')\n",
    "        # c = [1,1,1,1,1]\n",
    "        fit_parameters = dict({'a1': c[0],'k1': c[1],'a2': c[2],'k2':c[3],'b': c[0]+c[2]})\n",
    "        fit_parameters_sigma = dict({'a1': c[0],'k1': c[1],'a2': c[2],'k2':c[3], 'b': c[0]+c[2]})    # Send the parameters to a dictionary (reduces clutter)\n",
    "        self.ccov = ccov\n",
    "        # Calculates x and y values for the fitted curve\n",
    "        xdata_fitcurve = np.linspace(xdata_fit.min(), xdata_fit.max(), len(xdata_fit))    # Creates numpy aray with limits of the dataset for calculating the fitted curve\n",
    "        ydata_fitcurve = biphase_decay_func(c, xdata_fit)                         # Creates y-values for the fitted curve using the \n",
    "\n",
    "        # Add r square of the data and fitted curve to the parameter dict\n",
    "        fit_parameters.update({'rsq': r2_score(ydata_fit, ydata_fitcurve)})\n",
    "\n",
    "        if fit_parameters.get('k1')>fit_parameters.get('k2'):\n",
    "            fit_parameters['t(1/2) fast'] = np.log(2)/fit_parameters.get('k1')\n",
    "            fit_parameters['t(1/2) slow'] = np.log(2)/fit_parameters.get('k2')\n",
    "            \n",
    "            fit_parameters['tau fast'] = 1/fit_parameters.get('k1')\n",
    "            fit_parameters['tau slow'] = 1/fit_parameters.get('k2')\n",
    "\n",
    "            fit_parameters_sigma['t(1/2) fast'] = np.log(2)/fit_parameters_sigma.get('k1')\n",
    "            fit_parameters_sigma['t(1/2) slow'] = np.log(2)/fit_parameters_sigma.get('k2')\n",
    "            \n",
    "            fit_parameters_sigma['tau fast'] = 1/fit_parameters_sigma.get('k1')\n",
    "            fit_parameters_sigma['tau slow'] = 1/fit_parameters_sigma.get('k2')\n",
    "\n",
    "        else:\n",
    "            fit_parameters['t(1/2) fast'] = np.log(2)/fit_parameters.get('k2')\n",
    "            fit_parameters['t(1/2) slow'] = np.log(2)/fit_parameters.get('k1')\n",
    "\n",
    "            fit_parameters['tau fast'] = 1/fit_parameters.get('k2')\n",
    "            fit_parameters['tau slow'] = 1/fit_parameters.get('k1')\n",
    "\n",
    "            fit_parameters_sigma['t(1/2) fast'] = np.log(2)/fit_parameters_sigma.get('k2')\n",
    "            fit_parameters_sigma['t(1/2) slow'] = np.log(2)/fit_parameters_sigma.get('k1')\n",
    "\n",
    "            fit_parameters_sigma['tau fast'] = 1/fit_parameters_sigma.get('k2')\n",
    "            fit_parameters_sigma['tau slow'] = 1/fit_parameters_sigma.get('k1')\n",
    "\n",
    "        self.json_update('log', 'Two phase exponential decay fit has been applied')\n",
    "\n",
    "        # Pass to the fit_parameters dict, how many samples are considered for the (statistical-)analysis\n",
    "        fit_parameters.update({'sample-count': sample_count})\n",
    "\n",
    "        # Pass some variables to the object\n",
    "        self.sample_count = sample_count\n",
    "        self.df_sum_data = df_sum_data\n",
    "        self.df_sum_data_desc = df_sum_data_desc\n",
    "        self.fit_parameters = fit_parameters\n",
    "        self.fit_parameters_sigma = fit_parameters_sigma\n",
    "        self.xdata_fit = xdata_fit\n",
    "        self.ydata_fit = ydata_fit\n",
    "        self.xdata_fitcurve = xdata_fitcurve\n",
    "        self.ydata_fitcurve = ydata_fitcurve\n",
    "\n",
    "        self.json_update('raw sum data', [xdata_fit.tolist(), ydata_fit.tolist()])\n",
    "        self.json_update('2P exp fit data', [xdata_fitcurve.tolist(), ydata_fitcurve.tolist()])\n",
    "        self.json_update('confidence interval', [self.df_sum_data_desc['conf low'].tolist(), self.df_sum_data_desc['conf high'].tolist()])\n",
    "        self.json_update('fit parameters', fit_parameters)\n",
    "        self.json_update('log', 'Fit protocol finished')\n",
    "        self.to_json()\n",
    "\n",
    "    def json_update(self, key, value):\n",
    "        if key.lower() == 'log':\n",
    "            self.json_masterfile['log'].append(f'[{datetime.now().strftime(\"%H:%M:%S\")}]: ' + value)\n",
    "        elif type(value).__module__ == np.__name__:\n",
    "            self.json_masterfile.update({key : value.tolist()})\n",
    "        elif type(value) in [np.dtype(np.int64).type, np.dtype(np.float64).type, type(pd.Int64Dtype())]:\n",
    "            self.json_masterfile.update({key : float(value)})\n",
    "        else:    \n",
    "            self.json_masterfile.update({key: value})\n",
    "        \n",
    "    \n",
    "    def plots(self):\n",
    "        mpl.rcParams[\"axes.prop_cycle\"] = cycler(color=['#F6493E', '#A53EF6', '#EE4F9C', '#FEC953', '#99CC99', '#4DA64D', '#56C4C1', '#0D6973', '#2C3F59'])\n",
    "        plt.style.use('ggplot')\n",
    "        fig, (ax1, ax2) = plt.subplots(1,2,figsize=(20,5))\n",
    "        ax1.scatter(self.xdata_fit, self.ydata_fit, color='black', s=5)\n",
    "        ax1.fill_between(self.xdata_fit, self.df_sum_data_desc['conf low'][10:self.row_break_index], self.df_sum_data_desc['conf high'][10:self.row_break_index], color='black', alpha=0.3, label='SEM')\n",
    "        ax1.plot(self.xdata_fitcurve, self.ydata_fitcurve, '-', color='#F6493E', label=f'Two phase exponental decay fit \\n $R^2 = ${round(self.fit_parameters[\"rsq\"], 4)} \\n n = {self.sample_count}')\n",
    "        ax1.set_title('Fluorescent decay by diffusion')\n",
    "        ax1.set_xlabel('Time post photoactivation [s]')\n",
    "        ax1.set_ylabel(r'Normalized Relative fluorescent intensity $F/F_{max}$')\n",
    "        # ax1.axvline(self.fit_parameters['t(1/2) slow'],color='grey', linestyle=':', label=f'$t(1/2) =${round(self.fit_parameters[\"t(1/2) slow\"],2)}s')\n",
    "        ax1.legend(labelcolor='black')\n",
    "        for exp in self.df_sum_data:\n",
    "            ax2.plot(self.xdata_fit, self.df_sum_data[exp][10:self.row_break_index], alpha=0.6)\n",
    "        plt.savefig(os.path.join(self.export_path, f'fit_{os.path.basename(self.batch_csv)}.pdf'))\n",
    "        plt.ioff()\n",
    "\n",
    "\n",
    "    def add_experiment(self, protein_name, embryonic_stage, experiment_path, batch_file_mode=False, molecular_weight=0, disorder=0):\n",
    "        \"\"\"One experiment is added to the dict of experiments  \n",
    "        Embryonic Stage (int): hours post fertilization\n",
    "        Molecular weight in kDa\n",
    "        \"\"\"\n",
    "        exp = ExperimentGroup(experiment_path)\n",
    "        exp_name = protein_name + f' {embryonic_stage}hpf'\n",
    "        exp.mw = molecular_weight\n",
    "        exp.disorder_percent = disorder\n",
    "        exp.json_update('Name', exp_name)\n",
    "        if batch_file_mode == False:\n",
    "            exp.iterator(experiment_path)\n",
    "        elif batch_file_mode == True:    \n",
    "            exp.iterate()\n",
    "        exp.calculate_diffusivity()\n",
    "        exp.plots()\n",
    "\n",
    "        self.dict_experiments.update({exp_name : [protein_name, embryonic_stage, exp]})\n",
    "        self.json_update('log', f'Experiment \"{exp_name}\" has been added to the experiment group')\n",
    "\n",
    "    # TODO: Impement a selection based on age and protein as in plot_experiments()\n",
    "    def plot_molecular_weight(self, include_disorder=False, mega_graph=False):\n",
    "\n",
    "        something: fig = plt.figure(figsize=(20,20))\n",
    "        ax1 = fig.add_subplot(2,1,1)\n",
    "        \n",
    "        \n",
    "        if include_disorder == True:\n",
    "            ax2 = fig.add_subplot(2,1,2, projection='3d')\n",
    "        if mega_graph == True:\n",
    "                fig2 = plt.figure(figsize=(20,20))\n",
    "                ax3 = fig2.add_subplot(321, projection='3d')\n",
    "                ax4 = fig2.add_subplot(322)\n",
    "                ax5 = fig2.add_subplot(324)\n",
    "        for protein, experiment in self.dict_experiments.items():\n",
    "            name, hpf, exp_obj = experiment\n",
    "            color = self.get_protein_color(name, 'early' if hpf < 15 else 'late')\n",
    "            yer = exp_obj.fit_parameters_sigma['tau slow']/np.sqrt(exp_obj.sample_count)\n",
    "            ax1.scatter(exp_obj.mw, exp_obj.fit_parameters['tau slow'], label=f'{name} at {hpf} hpf', color=color)\n",
    "            ax1.errorbar(exp_obj.mw, exp_obj.fit_parameters['tau slow'], yerr=yer, capsize=2,color=color, alpha=0.75)\n",
    "            ax1.legend()\n",
    "            ax1.set_xlabel('Molecular weight [kDa]')\n",
    "            ax1.set_ylabel(r'$\\tau$-value [s] (as measure for diffusion)')\n",
    "            plt.savefig(os.path.join(self.export_path, 'mw_diffusion.pdf'))\n",
    "            if include_disorder == True:\n",
    "                print('x: ', exp_obj.mw, 'y: ', exp_obj.fit_parameters['tau slow'], 'z: ', exp_obj.disorder_percent)\n",
    "                ax2.stem([exp_obj.mw], [exp_obj.fit_parameters['tau slow']], [exp_obj.disorder_percent], label=f'{name} at {hpf} hpf' )\n",
    "            if mega_graph == True:\n",
    "                mw = exp_obj.mw\n",
    "                tau = exp_obj.fit_parameters['tau slow']\n",
    "                do = exp_obj.disorder_percent\n",
    "                \n",
    "                fig = plt.figure(figsize=(20,20))\n",
    "                \n",
    "                ax3.scatter(tau, mw, do)\n",
    "                ax3.set_title('3D-Scatterplot')\n",
    "                ax3.set_xlabel('diffusivity [s]')\n",
    "                ax3.set_ylabel('Molecular weight [kDa]')\n",
    "                ax3.set_zlabel('disorder')\n",
    "\n",
    "                \n",
    "                ax4.set_title('Scatter plot size representation of disorder')\n",
    "                ax4.scatter(mw, tau, s=1000*do)\n",
    "                ax4.set_ylabel('diffusivity [s]')\n",
    "                ax4.set_xlabel('Molecular weight [kDa]')\n",
    "\n",
    "                \n",
    "                ax5.set_title('Scatter plot size representation of ')\n",
    "                ax5.scatter(mw, do, s=50*tau)\n",
    "                ax5.set_ylabel('Percentage of diorder')\n",
    "                ax5.set_xlabel('Molecular weight [kDa]')\n",
    "                plt.savefig(os.path.join(self.export_path, 'mw_diffusion2.pdf'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def plot_experiments(self, experiments, developmental_stage=[], prompt=False, plot_all=False, save_path=None, plot_fit=True):\n",
    "        \"\"\"\n",
    "        This is a selector for ExperimentGroup.plot_multiple().  \n",
    "        \n",
    "        experiments: list of experiment names. Usually consist of protein name + XXhpf  \n",
    "        developmental_stage: int, default=None. Sorts for a special developmental stage.  \n",
    "    \n",
    "        prompt: boolean, default=False. If True, request the input of experiments from the user. Names should be sepereated be commata.  \n",
    "        \n",
    "        plot_all: boolean, default=False. If True, plots all Experiments, that were loaded with add_experiment()\n",
    "\n",
    "        \"\"\"\n",
    "        pass_on_dict = {}\n",
    "        if prompt == True or experiments == [] and plot_all == False:\n",
    "            for x in self.dict_experiments.keys():\n",
    "                print(x) \n",
    "            experiments = [string.lstrip() for string in input('Type in names of experiments you want to plot, separated by commata').split(',')]\n",
    "        elif plot_all == True:\n",
    "            experiments = list(self.dict_experiments.keys())\n",
    "\n",
    "        for experiment in experiments:\n",
    "            try:\n",
    "                if experiment in self.dict_experiments.keys() and developmental_stage == []:\n",
    "                    pass_on_dict.update({experiment : self.dict_experiments[experiment]})\n",
    "                elif experiment in self.dict_experiments.keys() and developmental_stage != []:\n",
    "                    if self.dict_experiments[experiment][1] in developmental_stage:\n",
    "                        pass_on_dict.update({experiment : self.dict_experiments[experiment]})\n",
    "                elif experiment not in self.dict_experiments.keys():\n",
    "                    raise ExperimentExistError(experiment, list(self.dict_experiments.keys()))\n",
    "                else:\n",
    "                    raise Exception\n",
    "            \n",
    "            except ExperimentExistError:\n",
    "                pass\n",
    "        \n",
    "        self.plot_multiple(pass_on_dict,\n",
    "        save_path=save_path,\n",
    "        plotfit=plot_fit\n",
    "        )\n",
    "\n",
    "    \n",
    "    def plot_multiple(self, selection_dict, save_path=None, plotfit=True):\n",
    "        colors=['#F6493E', '#A53EF6', '#EE4F9C', '#FEC953', '#99CC99', '#4DA64D', '#56C4C1', '#0D6973', '#2C3F59']\n",
    "        fig, ax = plt.subplots(figsize=(15,5))\n",
    "        a = 0\n",
    "        for key, item in selection_dict.items():\n",
    "            xp = item[2]\n",
    "            tau_sem = xp.fit_parameters_sigma[\"tau slow\"]/np.sqrt(xp.sample_count)\n",
    "            color = self.get_protein_color(item[0], 'early' if item[1] < 15 else 'late')\n",
    "            fle_name = str(key).replace(' ', '_')\n",
    "            fit_x, fit_y = xp.json_masterfile['2P exp fit data']\n",
    "            plot_x, plot_y = xp.json_masterfile[\"raw sum data\"]\n",
    "            conf_intv_low, conf_intv_high = xp.json_masterfile['confidence interval']\n",
    "            ax.fill_between(fit_x, conf_intv_low[10:xp.row_break_index],conf_intv_high[10:xp.row_break_index], alpha=0.5, color=color)\n",
    "            if plotfit == True:\n",
    "                ax.plot(fit_x, fit_y, color=color, ls='--',label=f'{str(key)} \\n'+r'$\\tau _{slow}=$'+f'{round(xp.fit_parameters[\"tau slow\"],2)}s +- {round(tau_sem,2)}', alpha=0.7)\n",
    "            ax.plot(plot_x, plot_y, color=color)\n",
    "            # ax.axvline(xp.fit_parameters['t(1/2) slow'],color=color, linestyle=':', label=r'$\\tau _{slow}=$'+f'{round(xp.fit_parameters[\"tau slow\"],2)}s +- {round(tau_sem,2)}')\n",
    "            ax.set_xlabel('Time post-bleaching [s]')\n",
    "            ax.set_ylabel('Normalized fluorescence intensity')\n",
    "            ax.set_xlim(-1,65)\n",
    "            a += 1\n",
    "        ax.legend(loc='upper right')\n",
    "        if save_path != None:\n",
    "            plt.savefig(os.path.join(save_path, 'summarized_graph.pdf'))\n",
    "\n",
    "    def compare_timepoints(self):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def get_protein_color(self, protein, developmental_stage=None, PrA_repeats=0):\n",
    "        \"\"\"\n",
    "        Developmental stage accepts 'early' and 'late'\n",
    "        \"\"\"\n",
    "        # Sets for colors\n",
    "        violet = ['#6147BF', '#412F80', '#201840', '#1A1333', '#1A1333']\n",
    "        blue1 = ['#6D79D6', '#4D5596', '#2C3157', '#252949', '#1A1D33']\n",
    "        petrol = ['#0ED7E6', '#0A9BA6', '#066066', '#055459', '#043C40']\n",
    "        green = ['#0ABF89', '#06805B', '#067353', '#045940', '#033324']\n",
    "        teal = ['#04CF8E', '#04C285', '#03A874', '#03825A', '#01422E']\n",
    "        lgreen = ['#B5E675', '#ABD96E', '#97BF62', '#79994E', '#46592E']\n",
    "        orange = ['#FBBF54', '#E0A94A', '#BA8C3D', '#7A5C28', '#3B2C13']\n",
    "        dorange = ['#FA703E', '#EE6B3B', '#D45F35', '#AD4E2B', '#6E311B']\n",
    "        red = ['#FA114C', '#EC1047', '#D40F40', '#AD0C34', '#6E0821']\n",
    "        purpur = ['#ED4088', '#AD2F64', '#A02C5D', '#87244E', '#611A38']\n",
    "        violet2 = ['#BD08A2', '#7D056B', '#700560', '#57034A', '#4A033F']\n",
    "        blue2 = ['#0448C7', '#033187', '#022C7A', '#022361', '#021F54']\n",
    "\n",
    "        protein_colors = {\n",
    "            'nanos, nos' : purpur,\n",
    "            'full vasa, full length vasa' : dorange,\n",
    "            'hypergerm, vasa(AA1-164)' : orange,\n",
    "            'Dead end, Dnd' : green,\n",
    "            'Granulito, gra' : blue1,\n",
    "            'Tdrd7' : violet,\n",
    "            'Bucky ball, buc' : teal,\n",
    "            'piwil1' : lgreen,\n",
    "            'Dazl' : blue2,\n",
    "            'Dazl F91A' : violet2\n",
    "        }\n",
    "        \n",
    "        colormode = {\n",
    "            'early' : 1,\n",
    "            'late' : 3,\n",
    "            'PrA' : [0, 1, 2, 3, 4]\n",
    "        }\n",
    "        \n",
    "        for prtn in protein_colors.keys():\n",
    "            if protein.upper() in prtn.upper():# and developmental_stage != None:\n",
    "                color = protein_colors[prtn][colormode[developmental_stage]]\n",
    "                break\n",
    "            else:\n",
    "                color = \"#000000\"\n",
    "\n",
    "        return color\n",
    "\n",
    "    #@classmethod\n",
    "    def experiment_with_control(self, batch_file_mode = False):\n",
    "        time_start = time.perf_counter()\n",
    "        # Control protein (hypergerm)\n",
    "        ctrl = ExperimentGroup(self.ctrl_csv)\n",
    "        ctrl.json_update('Name', self.ctrl_name)\n",
    "        if batch_file_mode == False:\n",
    "            ctrl.iterator(self.ctrl_csv)\n",
    "        elif batch_file_mode == True:    \n",
    "            ctrl.iterate()\n",
    "        ctrl.calculate_diffusivity()\n",
    "        ctrl.plots()\n",
    "        time_end_ctrl = time.perf_counter()\n",
    "        self.json_update('log', f'Control ({self.ctrl_name}) name was completed')\n",
    "        self.ctrl = ctrl\n",
    "        print(f'------------- \\t Finished \"control\" in {round(time_end_ctrl-time_start ,3)} second(s)\\t -------------')\n",
    "        \n",
    "        \n",
    "        # Protein of interest is calculated here\n",
    "        poi = ExperimentGroup(self.batch_csv)\n",
    "        poi.json_update('Name', self.poi_name)\n",
    "        if batch_file_mode == False:\n",
    "            poi.iterator(self.batch_csv)\n",
    "        elif batch_file_mode == True:    \n",
    "            poi.iterate()\n",
    "        poi.calculate_diffusivity()\n",
    "        poi.plots()\n",
    "        self.json_update('log', f'Control ({self.poi_name}) name was completed')\n",
    "        self.poi = poi\n",
    "        time_end_poi = time.perf_counter()\n",
    "        time_end_total = time.perf_counter()\n",
    "\n",
    "        \n",
    "        print(f'------------- \\t Finished \"Protein of interest\" in {round(time_end_poi-time_end_ctrl ,3)} second(s)\\t -------------')\n",
    "        print(f'------------- \\t Finished script in {round(time_end_total-time_start ,3)} second(s)\\t -------------')\n",
    "\n",
    "        ctrl_fle_name = self.ctrl_name.replace(' ', '_')\n",
    "        poi_fle_name = self.poi_name.replace(' ', '_')\n",
    "        fig, ax = plt.subplots(figsize=(15,5))\n",
    "        ctrl_fit_x, ctrl_fit_y = ctrl.json_masterfile['2P exp fit data']\n",
    "        ctrl_conf_intv_low, ctrl_conf_intv_high = ctrl.json_masterfile['confidence interval']\n",
    "        poi_fit_x, poi_fit_y = poi.json_masterfile['2P exp fit data']\n",
    "        poi_conf_intv_low, poi_conf_intv_high = poi.json_masterfile['confidence interval']\n",
    "        ax.fill_between(ctrl_fit_x, ctrl_conf_intv_low[10:ctrl.row_break_index], ctrl_conf_intv_high[10:ctrl.row_break_index], alpha=0.5, color='#cb95f5')\n",
    "        ax.plot(ctrl_fit_x, ctrl_fit_y, color='#A53EF6', label=f'{self.ctrl_name}')\n",
    "        ax.axvline(ctrl.fit_parameters['t(1/2) slow'],color='#cb95f5', linestyle=':', label=r'$\\tau _{slow}=$'+f'{round(ctrl.fit_parameters[\"tau slow\"],2)}s')\n",
    "        ax.fill_between(poi_fit_x, poi_conf_intv_low[10:poi.row_break_index],poi_conf_intv_high[10:poi.row_break_index], alpha=0.5, color='#456569')\n",
    "        ax.plot(poi_fit_x, poi_fit_y, color='#0D6973', label=f'{self.poi_name}')\n",
    "        ax.axvline(poi.fit_parameters['t(1/2) slow'],color='#456569', linestyle=':', label=r'$\\tau _{slow}=$'+f'{round(poi.fit_parameters[\"tau slow\"],2)}s')\n",
    "        ax.set_xlabel('Time after photobleaching [s]')\n",
    "        ax.set_ylabel('Normalized fluorescence intensity')\n",
    "        ax.set_xlim(-1,65)\n",
    "        ax.legend(loc='upper right')\n",
    "        plt.savefig(os.path.join(self.export_path, f'{ctrl_fle_name}_and_{poi_fle_name}.pdf'))\n",
    "        table_dict = {}\n",
    "        for key, value in self.ctrl.fit_parameters.items():\n",
    "            table_dict.update({key : [self.ctrl.fit_parameters[key].round(3), self.poi.fit_parameters[key].round(3)]})\n",
    "        cell_text = []\n",
    "        row_lbls = []\n",
    "        for key, value in table_dict.items():\n",
    "            row_lbls.append(key)\n",
    "            cell_text.append(value)\n",
    "        column_lbls = [self.ctrl_name, self.poi_name]\n",
    "        table = mpl.table.table(ax,\n",
    "            cellText=cell_text,\n",
    "            rowLabels=row_lbls,\n",
    "            colLabels=column_lbls,\n",
    "            loc='lower right',\n",
    "            colColours=['#cb95f550', '#45656950'],\n",
    "            colWidths=[0.1,0.1]\n",
    "            )\n",
    "        plt.savefig(os.path.join(self.export_path, f'{ctrl_fle_name}_and_{poi_fle_name}2.pdf'))\n",
    "\n",
    "\n",
    "\n",
    "    def experiment_without_control(self, batch_file_mode = False, root_path=''):\n",
    "        time_start = time.perf_counter()\n",
    "        if batch_file_mode == True:\n",
    "            self.iterate() # works with batch_file\n",
    "        elif batch_file_mode == False:\n",
    "            self.iterator(root_path)\n",
    "        self.calculate_diffusivity()\n",
    "        self.plots()\n",
    "        time_end = time.perf_counter()\n",
    "        print(f'------------- \\t Finished in {round(time_end-time_start ,3)} second(s)\\t -------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "totaltime_pre = time.perf_counter()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/YOMI/data/analyzed_data/diffusivity/FRAP_hypergerm/10hpf'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/62/yv6r5_y115s_tyrp10l5d9z40000gp/T/ipykernel_86697/4100155917.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mfrap\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mExperimentGroup\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'/Volumes/YOMI/data/analyzed_data/diffusivity/'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mfrap\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_experiment\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Vasa(AA1-164)'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'/Volumes/YOMI/data/analyzed_data/diffusivity/FRAP_hypergerm/10hpf'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmolecular_weight\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m16.87\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdisorder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m'/Volumes/YOMI/data/analyzed_data/diffusivity/FRAP_hypergerm'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mfrap\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_experiment\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Vasa(AA1-164)'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m24\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'/Volumes/YOMI/data/analyzed_data/diffusivity/FRAP_hypergerm/24hpf'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmolecular_weight\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m16.87\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdisorder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/62/yv6r5_y115s_tyrp10l5d9z40000gp/T/ipykernel_86697/1197110628.py\u001B[0m in \u001B[0;36madd_experiment\u001B[0;34m(self, protein_name, embryonic_stage, experiment_path, batch_file_mode, molecular_weight, disorder)\u001B[0m\n\u001B[1;32m    279\u001B[0m         \u001B[0mexp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson_update\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Name'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexp_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    280\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mbatch_file_mode\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 281\u001B[0;31m             \u001B[0mexp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexperiment_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    282\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mbatch_file_mode\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    283\u001B[0m             \u001B[0mexp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miterate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/62/yv6r5_y115s_tyrp10l5d9z40000gp/T/ipykernel_86697/1197110628.py\u001B[0m in \u001B[0;36miterator\u001B[0;34m(self, root_path)\u001B[0m\n\u001B[1;32m     61\u001B[0m                 \u001B[0mclear_output\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_json\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_json\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/62/yv6r5_y115s_tyrp10l5d9z40000gp/T/ipykernel_86697/1197110628.py\u001B[0m in \u001B[0;36mto_json\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     18\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m             \u001B[0;31m# This fuction is supposed to be passed at the end of ever other function to be certain ensure proper documentation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 20\u001B[0;31m             \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_csv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreplace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'.csv'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'.json'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'w'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mjson_file\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     21\u001B[0m                 \u001B[0mjson\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson_masterfile\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mjson_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindent\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mIsADirectoryError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Volumes/YOMI/data/analyzed_data/diffusivity/FRAP_hypergerm/10hpf'"
     ]
    }
   ],
   "source": [
    "frap = ExperimentGroup('/Volumes/YOMI/data/analyzed_data/diffusivity/',)\n",
    "frap.add_experiment('Vasa(AA1-164)', 10, '/Volumes/YOMI/data/analyzed_data/diffusivity/FRAP_hypergerm/10hpf', molecular_weight=16.87, disorder=1)\n",
    "'/Volumes/YOMI/data/analyzed_data/diffusivity/FRAP_hypergerm'\n",
    "frap.add_experiment('Vasa(AA1-164)', 24, '/Volumes/YOMI/data/analyzed_data/diffusivity/FRAP_hypergerm/24hpf', molecular_weight=16.87, disorder=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frap.add_experiment('Full length Vasa', 10, '/Volumes/YOMI/data/analyzed_data/diffusivity/FRAP_full_vasa/10hpf', molecular_weight=76.78, disorder=0.4056)\n",
    "frap.add_experiment('Full length Vasa', 24, '/Volumes/YOMI/data/analyzed_data/diffusivity/FRAP_full_vasa/24hpf', molecular_weight=76.78, disorder=0.4056)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frap.add_experiment('Dead end', 10, '/Volumes/YOMI/data/analyzed_data/diffusivity/FRAP_dnd/10hpf', molecular_weight=46.0, disorder=0.2871)\n",
    "frap.add_experiment('Dead end', 24, '/Volumes/YOMI/data/analyzed_data/diffusivity/FRAP_dnd/24hpf', molecular_weight=46.0, disorder=0.2871)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frap.add_experiment('Dazl', 10, '/Volumes/YOMI/data/analyzed_data/diffusivity/FRAP_dazl/10hpf', molecular_weight=25.51, disorder=0.4760)\n",
    "frap.add_experiment('Dazl F91A', 10, '/Volumes/YOMI/data/analyzed_data/diffusivity/FRAP_mutdazl/10hpf', molecular_weight=25.51, disorder=0.4760)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frap.add_experiment('Granulito', 10, '/Volumes/YOMI/data/analyzed_data/diffusivity/FRAP_gra/10hpf', molecular_weight=16.68, disorder=0.2979)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "frap.plot_experiments(\n",
    "    ['Vasa(AA1-164) 10hpf', 'Full length Vasa 10hpf', 'Dead end 10hpf', 'Granulito 10hpf'],\n",
    "    developmental_stage=[10],\n",
    "    plot_all=False,\n",
    "    save_path='/Users/izbuser/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master/_Raz Lab/_Presentations/PR/20220900_PR_PhD-1/source images/diff_plots',\n",
    "    plot_fit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "totaltime_post = time.perf_counter()\n",
    "totaltime = totaltime_post-totaltime_pre\n",
    "print(totaltime)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frap.dict_experiments.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_graphs(names, save_name):\n",
    "    taus = []\n",
    "    taus_err = []\n",
    "    # x = []\n",
    "    colors = []\n",
    "    ages = []\n",
    "    proteins = []\n",
    "    for name in names:\n",
    "        xxp = frap.dict_experiments[name][2]\n",
    "        protein = frap.dict_experiments[name][0]\n",
    "        tau = xxp.fit_parameters['tau slow']\n",
    "        hpf = frap.dict_experiments[name][1]\n",
    "        err = xxp.fit_parameters_sigma['tau slow']/np.sqrt(xxp.sample_count)\n",
    "        color = frap.get_protein_color(protein, 'early' if hpf < 15 else 'late')\n",
    "        colors.append (color)\n",
    "        taus.append(tau)\n",
    "        proteins.append(protein)\n",
    "        taus_err.append(err)\n",
    "        # ages.append(f'{hpf} hpf \\n n={int(xxp.sample_count)}')\n",
    "        ages.append(f'{protein} \\n {hpf} hpf')\n",
    "    xval = [a for a in range(len(names))]\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(xval, taus, yerr=taus_err, color=colors, alpha=0.85, tick_label=ages, width=0.75, capsize=3, ecolor='#231F20')\n",
    "    ax.set_ylabel(r'Difffusivity  ($\\tau$ [s])')\n",
    "    # ax.legend()\n",
    "    plt.savefig('/Users/izbuser/Library/Mobile Documents/com~apple~CloudDocs/Documents/Master/_Raz Lab/_Presentations/PR/20220900_PR_PhD-1/source images/tau_plots' + os.path.sep + save_name + '.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_graphs(['Vasa(AA1-164) 10hpf', 'Vasa(AA1-164) 24hpf'], 'hyper1024')\n",
    "bar_graphs(['Full length Vasa 10hpf', 'Full length Vasa 24hpf'], 'vs10254')\n",
    "bar_graphs(['Vasa(AA1-164) 10hpf', 'Full length Vasa 10hpf', 'Dead end 10hpf', 'Granulito 10hpf'], 'all')\n",
    "bar_graphs(['Vasa(AA1-164) 10hpf', 'Full length Vasa 10hpf', 'Dead end 10hpf'], 'early')\n",
    "bar_graphs(['Vasa(AA1-164) 24hpf', 'Full length Vasa 24hpf', 'Dead end 24hpf'], 'late')\n",
    "bar_graphs(['Dead end 10hpf', 'Dead end 24hpf'], 'Dnd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frap.plot_molecular_weight(include_disorder=False, mega_graph=False)\n",
    "frap.get_protein_color('nanos', 'early')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, hpf, xxp = frap.dict_experiments['Vasa(AA1-164) 10hpf']\n",
    "tau = xxp.fit_parameters['tau slow']\n",
    "mw = xxp.mw\n",
    "do = xxp.disorder_percent\n",
    "\n",
    "print(do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "ax1 = fig.add_subplot(321, projection='3d')\n",
    "ax1.scatter(tau, mw, do)\n",
    "ax1.set_title('3D-Scatterplot')\n",
    "ax1.set_xlabel('diffusivity [s]')\n",
    "ax1.set_ylabel('Molecular weight [kDa]')\n",
    "ax1.set_zlabel('disorder')\n",
    "\n",
    "ax2 = fig.add_subplot(322)\n",
    "ax2.set_title('Scatter plot size representation of disorder')\n",
    "ax2.scatter(mw, tau, s=1000*do)\n",
    "ax2.set_ylabel('diffusivity [s]')\n",
    "ax2.set_xlabel('Molecular weight [kDa]')\n",
    "\n",
    "ax3 = fig.add_subplot(324)\n",
    "ax3.set_title('Scatter plot size representation of ')\n",
    "ax3.scatter(mw, do, s=50*tau)\n",
    "ax3.set_ylabel('Percentage of diorder')\n",
    "ax3.set_xlabel('Molecular weight [kDa]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def diff(mw1, mw2):\n",
    "#     # d = np.sqrt(mw1)/np.sqrt(mw2)\n",
    "#     d = mw1 + mw2\n",
    "#     return d\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "# v=1\n",
    "# for clr in [violet, blue1, petrol, green, teal, lgreen, orange, dorange, red, purpur, violet2, blue2]:\n",
    "#     x = []\n",
    "#     y = []\n",
    "#     z = []\n",
    "#     f = []\n",
    "#     for a in range(10, 150):\n",
    "#         x.append(a)\n",
    "#         y.append(diff(a, v))\n",
    "#     ax.plot(x, y, color=clr[0])\n",
    "#     for b in range(290, 151, -1):\n",
    "#         z.append(b)\n",
    "#         f.append(diff(b, v))\n",
    "#     ax.plot(z, f, color=clr[3])\n",
    "#     v+=50\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xxx = []\n",
    "yyy = []\n",
    "\n",
    "for i in range(50):\n",
    "    xxx.append(i)\n",
    "    yyy.append(3*i**3+2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph = plt.plot(xxx, yyy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "current_version: 3.2.2\n",
    "---\n",
    "This miserable piece of crap was written by J.Wegner; for any questions adress: jwegner1@uni-muenster.de\n",
    "\n",
    "#### Version 1.0\n",
    "Extracts data from Fiji-exported .csv-files and plots the intenstity for one channel that can be selected - is directed towards data from dendra\n",
    "\n",
    "#### Version 2.1\n",
    "Version 2 introduces the comparison of columns with each other from multiple sources  \n",
    "Feature added for extraction of bleach time and timeintervals of acqusition from .lsm-files; requires the input of path from a .lsm-file\n",
    "\n",
    "#### Version 2.2\n",
    "Added the 'dataextractor' function allowing for easier change in numbers of datasets that can be fed into software  \n",
    "Added GUI functionality featureing a menu for easy guided way to select the lsm-path, csv-active, csv-passive and export-path  \n",
    "Added a way to plot as many analyzed/passive csv's as wished by the user following a plt.style-set colorscheme\n",
    "\n",
    "#### Version 2.3.0\n",
    "Added feature: export of a .csv-file containing compact information of the calculared (displayed/plotted) values for each condition  \n",
    "Added feature: animation for the activated channel; stops after about 4s\n",
    "\n",
    "#### Version 2.3.1\n",
    "Bug fix: Animated curve doesn't stop ater 100 frames anymore  \n",
    "Bug fix: The plot style 'ggdark' was adjusted to better fit dark presentations\n",
    "\n",
    "#### Version 2.3.2\n",
    "Bug fix: The tkinter window is now locating to the middle of the screen, but has a fixed heighth and width, which might cause problems\n",
    "\n",
    "#### Version 2.4 \n",
    "Bug fix: Title of graph and axes for main intensity plot  \n",
    "Introduciton of two phase exponential curve fitting and calculation of the diffusion coefficient  \n",
    "DEV note: The equation for calculating the diffusion coefficient is lacking the exact area information  \n",
    "DEV note: BATCHimport is work in progress\n",
    "\n",
    "\n",
    "#### Version 2.4.1\n",
    "Bug fix: The equation for the diffusion coefficient now has the correct area information  \n",
    "Introduction of a GUI that displays the parameter of the fit  \n",
    "Saving the fitting-parameter dictionary as CSV\n",
    "\n",
    "\n",
    "#### Version 2.4.2\n",
    "DEV note: Batch-mode is up and running; batch file is copied and fit_parameters is attached\n",
    "\n",
    "\n",
    "#### Version 2.4.3\n",
    "DEV note: Normalization according to Phair et al. - requires different input files, adjusted GUI and batch input\n",
    "\n",
    "\n",
    "#### Version 2.4.4\n",
    "Bug fix: Prebleach frames are not how they should be - seem kind of shifted. \n",
    "Bug fix: Doublchecked and made the bleach start/stop values \n",
    "\n",
    "\n",
    "#### Version 2.4.5\n",
    "FEATURE: Batch file only needs the lsm file and the secondary granule paths, the rest is autocompleted if the common naming scheme is followed\n",
    "\n",
    "#### Version 3.0\n",
    "DEV note: Code was rewritten to be more object oriented. One experiment becomes one object of the 'experiment' and all experiments are summarized in a ExperimentGroup object. This leaves room for a GUI and inside the GUI coupling of of experiments (such as ctrl and treatment conditions)  \n",
    "Feature: Summarizing all experiments is moved from an external jupyter nb to the main code. For the future 'napari' could be implemented to be independent from ImageJ for the analysis   \n",
    "Feature: Better documentation of what operations have been performed with json and saving also of more detailed information such as lists of x/y values at for various data. Therefore, substituting the batch_results '.csv'-file\n",
    "\n",
    "#### Version 3.1\n",
    "DEV note: Transferred to a Jupyter Notebook. Is potenialy worse for a GUI, but better for semi manual processing and testing  \n",
    "Feature: Introduced a built in comparision to a control (hypergerm), that is imaged in parallel\n",
    "\n",
    "#### Version 3.1.2\n",
    "DEV note: Grown independent of batchfiles by introducing another regex based iterator\n",
    "\n",
    "#### Version 3.2\n",
    "DEV note: New version supports FRAP but looses capability to do iFRAP. Reason for this is a difference in division (first frame after activation vs. last frame before bleaching)  \n",
    "BUG: Adjacent granules can not be calulated any more\n",
    "\n",
    "#### Version 3.2.1\n",
    "Feature: Now the class \"ExperimentGroup\" features methods \"add_experiment\", \"plot_multiple\" and \"plot_experiments\". add_experiments activates the iterator and which analyses a given folder. plot_experiments can be called (with or without prompt) it selects the experiments that are given as an argument and passes the information and passes it to plot_multiple, which plots multiple curves\n",
    "BUG: If a file or a column or file is missing the whole program is crashing. Possible fix: try except loop, which is noted in the json file and printed out in the summary\n",
    "\n",
    "#### Version 3.2.2\n",
    "BUG FIX: \n",
    "FEATURE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
